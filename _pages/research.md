---
title: "PHERL Lab - Research"
layout: textlay
excerpt: "Allan Lab -- Research"
sitemap: false
permalink: /research/
---

# Research
We are a Human-Robot Collaborative Control and Robot Learning research group, whose goal is to develop principled, structured methods to enable *seamless, adaptive human-robot interaction and collaboration*. 

## Principled Methods for Human-Robot Collaborative Learning and Control
Enabling seamless human-robot collaboration while ensuring task success requires the reduction of the task information to its essential features through representations that are intuitive for the user. To do this, I focus on developing methods for generating and learning task representations from motion data and information that can be incorporated into model-based control methods. My goal is to develop methods that enable autonomy to extract understandable task embeddings that enable control methods to ensure dynamic feasibility and provably safe behavior and can be communicated to and interpreted by humans, a step in developing explainable AI.

## Optimal Interface Design for Human-Robot Collaboration
For robot interfaces to be practical for humans, they must be simple, seamless, and mitigate cognitive load for the user. This is particularly critical for users who may not have familiarity with robotic systems or in settings with complex collaborative robotic systems (e.g., robot swarms) or dynamically changing environments, where managing the system can be overwhelming. In my research, I seek to design user interfaces for human-robot collaboration that effectively communicates information to the human with minimal cognitive load and training to use.

## Learning Interpretable Action-Perception Models
As robotic systems are deployed with varying sensor modalities, especially in novel scenarios, it is important for the robot to build models of how it interacts in its environment, e.g., a action- perception model, and be able to communicate its role and capabilities. I aim to develop methods that enable robots to autonomously construct meaningful sensor-environment models that are physically-driven (i.e., an model of their world and how their actions affect their perception of it) and can encode multimodal information into latent representations that are human-interpretable. Furthermore, in high-dimensional sensor systems, I explore how we can efficiently acquire and utilize the data for model learning and communication.

### ... and more.
